# Claude Analytics Framework

> AI-powered analytics development with specialized agents and workflows

A framework for analytics teams that combines Claude Code with specialized agents, patterns, and MCP integrations for your data stack.

**What you get:**
- 5 slash commands for project lifecycle (`/idea` ‚Üí `/research` ‚Üí `/start` ‚Üí `/switch` ‚Üí `/complete`)
- AI agents created based on YOUR tools (setup asks what you use)
- Optional live system access via MCP servers
- Automatic learning from completed projects

**Built on:** [Claude Code](https://docs.claude.com/en/docs/claude-code) (Anthropic's official AI coding assistant)

**üöÄ [Just want to try it? See QUICKSTART.md ‚Üí](QUICKSTART.md)**

---

## TL;DR - Is This For You?

**You'll love this if you want:**
- ü§ñ **AI agents specialized for YOUR stack** - knows your dbt patterns, warehouse setup, BI standards
- üìÇ **Organized project workflow** - structured folders, clear documentation, automated git workflow
- üîó **Coordinated cross-repo changes** - one project manages dbt + warehouse + BI changes together
- üìù **Team knowledge capture** - patterns extracted automatically, new team members get your expertise
- üîå **Live system integration** - MCP servers connect to dbt Cloud, Snowflake, GitHub for real-time context
- ‚ö° **Workflow automation** - `/idea` ‚Üí `/research` ‚Üí `/start` ‚Üí `/complete` commands guide your process

**What makes it different:**
- **Specialized agents**: Not generic AI - agents configured for your specific tools and patterns
- **Learning system**: After 3-5 projects, knows your incremental strategies, naming conventions, testing standards
- **Structured workflow**: Project organization, branch management, PR creation built-in
- **Context management**: `/switch` for multi-project work, or focus on single projects with full context
- **Tool-agnostic**: Pre-built for dbt/Snowflake/Tableau, easily extended for your stack

**Time investment:**
- ‚ö° Setup: 5 minutes (interactive questions about your stack)
- ‚ö° First project: Immediate (`/start "idea"` ‚Üí working folder)
- ‚ö° ROI: After project #3, agents know your patterns better than documentation

**When NOT to use this:**
- ‚ùå One-off quick fixes or simple scripts
- ‚ùå Single-tool projects (just dbt, just SQL, etc.)
- ‚ùå Ad-hoc data exploration
- ‚ùå Projects with <3 files or <1 hour of work

**This framework adds value when:**
- ‚úÖ Working on 3+ projects per month
- ‚úÖ Using multiple tools (dbt + warehouse + orchestration + BI)
- ‚úÖ Building similar things repeatedly (dashboards, pipelines, models)
- ‚úÖ Working in a team that needs to share patterns

---

## What This Is

**A meta-repository that orchestrates work across your data stack.**

Think of it as a control center sitting alongside your existing repositories (dbt projects, Tableau dashboards, data pipelines, etc.). It doesn't replace your repos - it provides structure and AI guidance for working on them.

**The architecture:**
```
claude-analytics-framework/          ‚Üê This repo (meta-layer)
‚îú‚îÄ‚îÄ .claude/                    ‚Üê AI agents + commands
‚îÇ   ‚îú‚îÄ‚îÄ agents/                 ‚Üê Specialists for YOUR tools
‚îÇ   ‚îú‚îÄ‚îÄ commands/               ‚Üê /idea, /start, /complete, etc.
‚îÇ   ‚îî‚îÄ‚îÄ config/                 ‚Üê Your tech stack configuration
‚îú‚îÄ‚îÄ projects/                   ‚Üê Active/completed project folders
‚îÇ   ‚îú‚îÄ‚îÄ active/                 ‚Üê Current work
‚îÇ   ‚îî‚îÄ‚îÄ completed/              ‚Üê Archived learnings
‚îú‚îÄ‚îÄ repos/                      ‚Üê Your actual data repos (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ dbt_cloud/              ‚Üê Your dbt project
‚îÇ   ‚îú‚îÄ‚îÄ data_pipelines/         ‚Üê Your pipeline code
‚îÇ   ‚îî‚îÄ‚îÄ tableau_workbooks/      ‚Üê Your BI dashboards
‚îî‚îÄ‚îÄ scripts/                    ‚Üê Workflow automation
```

**Key:** The `repos/` folder keeps your work repositories organized in one place. It's gitignored - these repos aren't committed to the framework.

**How it augments your development:**

1. **Structured coordination**: Instead of scattered work across repos, you get organized projects that coordinate changes
2. **AI that knows your stack**: Agents read from and operate on your actual repositories with context about your tools
3. **Cross-repo workflows**: Work that spans dbt + Snowflake + Tableau gets coordinated in one project
4. **Knowledge accumulation**: As you complete projects, the framework learns your patterns and improves recommendations

**The workflow:**
1. **Capture** ideas as GitHub issues (`/idea "build customer dashboard"`)
2. **Research** complex topics before building (`/research 123` - optional)
3. **Start** projects with organized folders (`/start 123` - creates structure + git branch)
4. **Switch** between projects seamlessly (`/switch` - zero-loss context switching)
5. **Complete** and archive (`/complete project-name` - closes issue, extracts learnings)

**The AI agents:**
- Setup asks what tools you use (dbt? Snowflake? Tableau? etc.)
- Creates specialist agents for your stack
- Agents know patterns for YOUR tools (not generic advice)
- Optionally connect to live systems (dbt Cloud API, Snowflake queries, etc.)

**The result:**
You still work in your normal repos, but now you have:
- Project organization and tracking
- AI assistance tailored to your tools
- Cross-repository coordination
- Automatic knowledge capture

**Example: Multi-repo project coordination**

```
Project: "Add customer churn prediction dashboard"

claude-analytics-framework/
‚îú‚îÄ‚îÄ projects/active/feature-churn-dashboard/
‚îÇ   ‚îú‚îÄ‚îÄ spec.md              ‚Üê What we're building
‚îÇ   ‚îú‚îÄ‚îÄ context.md           ‚Üê Current state, blockers, decisions
‚îÇ   ‚îî‚îÄ‚îÄ tasks/
‚îÇ       ‚îú‚îÄ‚îÄ dbt-expert-findings.md      ‚Üê Model designs
‚îÇ       ‚îú‚îÄ‚îÄ tableau-expert-findings.md  ‚Üê Dashboard layouts
‚îÇ       ‚îî‚îÄ‚îÄ data-engineer-findings.md   ‚Üê Pipeline setup
‚îÇ
‚îî‚îÄ‚îÄ repos/                              ‚Üê Your actual repos
    ‚îú‚îÄ‚îÄ dbt_cloud/
    ‚îÇ   ‚îî‚îÄ‚îÄ models/marts/customer_churn.sql     (new model)
    ‚îú‚îÄ‚îÄ data_pipelines/
    ‚îÇ   ‚îî‚îÄ‚îÄ prefect/churn_refresh.py            (new pipeline)
    ‚îî‚îÄ‚îÄ tableau_workbooks/
        ‚îî‚îÄ‚îÄ churn_dashboard.twb                  (new dashboard)

On /complete:
‚úÖ All changes committed to respective repos
‚úÖ Patterns extracted: "customer churn modeling approach"
‚úÖ Future churn projects now start with YOUR proven pattern
```

**[See a real example project ‚Üí](examples/sample-project/README.md)**

### The Workflow (Visual)

```
üí° Idea
  ‚Üì /idea
üìã GitHub Issue #123
  ‚Üì /research (optional)
üî¨ Research Report
  ‚Üì /start 123
üìÅ Project Folder + Git Branch + AI Agents
  ‚Üì Work with specialists
üîÑ /switch (context switch anytime)
  ‚Üì /complete
‚úÖ Archived + Learnings Extracted + Issue Closed
  ‚Üì
üíæ Memory System (future projects benefit)
```

---

## Quick Start (Zero to Running in 5 Minutes)

**Prerequisites**: You need [Claude Code](https://docs.claude.com/en/docs/claude-code) installed first.

### Never used Claude Code?

Claude Code is Anthropic's official AI coding assistant that runs in your terminal.

**Installation:** Follow the [official installation guide](https://docs.claude.com/en/docs/claude-code/installation)

### Installation

**Step 1: Clone this repository**
```bash
git clone https://github.com/dylpickledev/claude-analytics-framework.git
cd claude-analytics-framework
```

**Step 2: Run interactive setup**
```bash
./setup.sh
```

**What setup actually does (and doesn't do):**

‚úÖ **Does:**
- Asks 4 questions about your data stack
- Copies relevant agent template files to `.claude/agents/`
- Creates `.claude/config/tech-stack.json` with your answers
- Optionally guides MCP server setup (you can skip this)

‚ùå **Does NOT:**
- Modify your system settings
- Install packages globally
- Change your existing Claude Code configuration in other projects
- Touch your existing data repositories
- Require admin/sudo access

**Can you undo it?** Yes. Just delete this directory.
**Can you change your answers?** Yes. Re-run `./setup.sh` or edit files manually.
**Safe to test?** Yes. Try in a test directory first if nervous.

**What it asks:**

1. **Checks** you have Claude Code installed (exits with install instructions if not)
2. **Asks** about your tech stack:
   - What transformation tool? (dbt Cloud, dbt Core, raw SQL, etc.)
   - What data warehouse? (Snowflake, BigQuery, PostgreSQL, etc.)
   - What orchestration? (Prefect, Airflow, manual, etc.)
   - What BI tool? (Tableau, Power BI, Streamlit, etc.)
3. **Creates AI specialists** matching your answers:
   - If you said "dbt Cloud" ‚Üí creates `dbt-expert.md` agent
   - If you said "Snowflake" ‚Üí creates `snowflake-expert.md` agent
   - If you said "Tableau" ‚Üí creates `tableau-expert.md` agent
   - If you said "Other [tool]" ‚Üí offers to create custom agent from template
4. **Optionally configures MCP servers** for live data access (you can skip this):
   - dbt MCP ‚Üí query dbt Cloud API in real-time
   - Snowflake MCP ‚Üí execute warehouse queries directly
   - GitHub MCP ‚Üí analyze repositories and issues

**Step 3: Start your first project**
```bash
claude /start "your first project idea"
```

**That's it!** You have a project folder, AI agents that know your tools, and the 5-command workflow ready to use.

---

## The 5 Commands

### 1. `/idea` - Capture ideas
Creates GitHub issues for data initiatives
```bash
claude /idea "Optimize Snowflake costs"
```

### 2. `/research` - Deep exploration
Specialist agents analyze technical approach
```bash
claude /research "dbt incremental model strategy"
claude /research 123  # Analyze issue #123
```

### 3. `/start` - Begin development
Creates project structure + git branch
```bash
claude /start 123  # From issue
claude /start "new project idea"  # Creates issue + starts
```

### 4. `/switch` - Context switching
Zero-loss project switching with backup
```bash
claude /switch  # Save current, switch to main
claude /switch feature-branch  # Switch to specific branch
```

### 5. `/complete` - Finish project
Archives project, extracts learnings, closes issue
```bash
claude /complete feature-project-name
```

**Bonus Commands:**
- `/pause [description]` - Save conversation context for seamless resumption

**Note**: Use GitHub's native issue management, labels, and milestones for roadmap planning and prioritization.

---

## The AI Agents (Created During Setup)

### Role Agents (Always Created)

These handle 80% of work independently:

**analytics-engineer-role**
- Writes dbt models, SQL transformations, data modeling
- Optimizes query performance, semantic layer design
- Prepares data for BI consumption

**data-engineer-role**
- Sets up data pipelines and orchestration
- Integrates source systems, ensures data quality
- Chooses appropriate ingestion tools

**data-architect-role**
- Designs system architecture
- Makes technology selection decisions
- Plans cross-system integration

### Specialist Agents (Created Based on Your Tech Stack)

Setup creates these **only if you use these tools**:

**dbt-expert** (if you selected dbt Cloud or dbt Core)
- Deep dbt knowledge: incremental models, testing, packages
- Can connect to dbt Cloud API via MCP (optional setup)

**snowflake-expert** (if you selected Snowflake)
- Warehouse optimization and cost analysis
- Can query Snowflake directly via MCP (optional setup)

**tableau-expert** (if you selected Tableau)
- Dashboard performance and design patterns

**claude-code-expert** (always created)
- Helps with setup, troubleshooting, workflow questions

**Don't see your tool?** Setup offers to create a custom specialist agent from a template.

### Supported Tools (Out of the Box)

Setup recognizes these tools and creates appropriate specialists:

**Transformation:**
- dbt Cloud ‚úÖ
- dbt Core ‚úÖ
- Raw SQL / Stored Procedures (uses generic analytics-engineer-role)
- Databricks SQL (creates custom agent from template)
- Other (you create custom agent)

**Warehouses:**
- Snowflake ‚úÖ (with optional MCP)
- BigQuery (uses generic knowledge)
- PostgreSQL (uses generic knowledge)
- Databricks (uses generic knowledge)
- Redshift (uses generic knowledge)
- Other (you create custom agent)

**Orchestration:**
- Prefect (uses generic data-engineer-role)
- Airflow (uses generic data-engineer-role)
- Dagster (uses generic data-engineer-role)
- dbt Cloud scheduler (uses dbt-expert)
- Other (you create custom agent)

**BI/Visualization:**
- Tableau ‚úÖ
- Power BI (uses generic knowledge)
- Looker (uses generic knowledge)
- Streamlit (uses generic knowledge)
- Metabase (uses generic knowledge)
- Other (you create custom agent)

**Always Available:**
- GitHub ‚úÖ (with optional MCP)
- Git workflow (built-in)
- Claude Code expert (setup/troubleshooting)

‚úÖ = Has dedicated specialist agent with deep expertise
üîå = MCP server available for live system access

**The truth about "support":**
- ‚úÖ = Pre-built agents with mature patterns (fastest time to value)
- "Generic knowledge" = Claude's base knowledge works, agents guide workflow
- "Custom agent" = You create specialist, framework helps it improve over time

**This framework is designed to support ANY tool:**
- Start with basic agents (even generic ones work)
- Use the ADLC workflow to build projects
- Agents learn your patterns as you complete work
- After 3-5 projects, your custom agents know YOUR specific patterns
- The `/complete` command extracts learnings automatically

**Bottom line:** You don't need pre-built agents to start. The framework is self-improving. Your first projects teach the agents about your tools.

### How Agents Work Together

Role agents delegate to specialists for complex cases (20% of work). Example:

```
You: "Optimize this slow dbt model"
  ‚Üì
analytics-engineer-role: Reviews model structure
  ‚Üì (needs deep dbt expertise)
dbt-expert: Analyzes incremental strategy, suggests optimizations
  ‚Üì (if MCP configured)
dbt-mcp: Queries actual run history from dbt Cloud API
  ‚Üì
Result: Specific optimization based on YOUR production data
```

---

## MCP Integration (Optional)

MCP servers connect Claude to your live systems for real-time data access.

**Setup asks** if you want to configure these (you can skip and add later):

**dbt MCP** (if using dbt):
- Query model metadata and job status
- Analyze test results from actual runs
- Access Semantic Layer metrics

**Snowflake MCP** (if using Snowflake):
- Execute queries directly in your warehouse
- Analyze query performance and costs
- Check data quality in real tables

**GitHub MCP** (always offered):
- Search code across repositories
- Analyze issues and pull requests
- Review commit history

**How it works:**
1. Setup guides you through credential collection
2. Updates `claude_desktop_config.json` with MCP server config
3. Restart Claude Desktop ‚Üí MCP servers load automatically
4. Agents can now query your real systems

**Why it matters:** Instead of generic advice, agents see YOUR actual data and make informed recommendations.

---

## What You Need to Get Started

**Essential:**
- [Claude Code](https://docs.claude.com/en/docs/claude-code) - Anthropic's AI coding assistant
- Git - For version control and GitHub integration

**Your data tools:**
- The framework adapts to what YOU use
- Setup asks about your stack and configures accordingly
- No specific tools required upfront

**Optional (setup will guide you):**
- Node.js/npm - Only if you want MCP server integration
- GitHub CLI (`gh`) - For enhanced GitHub operations
- Docker + VS Code - If you want devcontainer setup (see below)

---

## Advanced Setup Options

### Devcontainer Setup (Fully Automated)

Get everything pre-installed in a Docker container:

```bash
# Prerequisites: Docker Desktop + VS Code + Dev Containers extension

git clone https://github.com/dylpickledev/claude-analytics-framework.git
cd claude-analytics-framework
code .

# VS Code will prompt "Reopen in Container" ‚Üí Click it
# Wait ~2-3 minutes for first-time setup

# Once container is ready:
gh auth login        # Authenticate with GitHub
claude auth          # Authenticate with Claude Code
./setup.sh           # Run interactive setup
```

**Devcontainer includes:**
- Claude Code pre-installed
- Node.js + npm (for MCP servers)
- GitHub CLI
- Git configured
- All dependencies ready

See [.devcontainer/README.md](.devcontainer/README.md) for details.

---

## Example Workflow

Here's what a typical project looks like:

```bash
# 1. Capture idea ‚Üí GitHub issue
claude /idea "Build customer analytics dashboard"
# Output: ‚úÖ Created issue #123

# 2. Research (optional - for complex projects)
claude /research 123
# Specialist agents analyze approach, feasibility, technical details
# You get a research report before starting

# 3. Start project ‚Üí folder + git branch + issue link
claude /start 123
# Output:
#   ‚úÖ Created: projects/active/feature-customer-analytics/
#   ‚úÖ Created branch: feature/customer-analytics
#   ‚úÖ Linked to issue #123
#   üìù Ready for development

# 4. Work with AI agents
claude "use analytics-engineer-role to design the data model"
# Role agent delegates to dbt-expert for complex modeling
# If MCP configured, queries your actual dbt project structure

claude "use data-engineer-role to set up incremental refresh"
# Gets suggestions based on your orchestration tool (from setup)

# 5. Context switch (urgent work pops up)
claude /switch  # Saves current work, returns to main
claude /start 125  # Different urgent project
claude /complete feature-urgent-fix  # Finish urgent work
claude /switch feature-customer-analytics  # Resume original work
# Zero loss - picks up exactly where you left off

# 6. Complete and learn
claude /complete feature-customer-analytics
# Output:
#   ‚úÖ Archived to: projects/completed/2025-10/feature-customer-analytics/
#   ‚úÖ Extracted learnings to memory system
#   ‚úÖ Closed GitHub issue #123
#   üí° Agents now know customer analytics patterns for future projects
```

**What just happened:**
- Idea tracked in GitHub (team visibility)
- Organized project folder (not scattered files)
- AI agents with domain expertise helped (not generic chatbot)
- Work saved when context switching (no lost progress)
- Knowledge extracted automatically (future projects benefit)

---

## What Makes This Different

**This is NOT:**
- ‚ùå A replacement for your existing repos
- ‚ùå A new tool you have to migrate to
- ‚ùå Just AI prompts in a folder
- ‚ùå Locked into specific tools

**This IS:**
- ‚úÖ A meta-repository that sits alongside your existing work
- ‚úÖ A structured workflow for coordinating cross-repo changes
- ‚úÖ A learning system that captures YOUR team's expertise
- ‚úÖ Tool-agnostic with agent templates for any stack

**How it's different from just using Claude Code:**

| Plain Claude Code | claude-analytics-framework |
|---|---|
| Ad-hoc conversations | Organized projects with tracking |
| Generic AI knowledge | Agents that know YOUR tools & patterns |
| Work scattered across repos | Coordinated cross-repo workflows |
| No memory between sessions | Automatic pattern extraction & learning |
| Manual git workflow | Built-in branch/PR workflow |
| Lost context when switching | Zero-loss context switching |

**Practical benefits:**

- **Works with your existing setup**: No migration, just add this repo alongside your others
- **Coordinates multi-repo work**: dbt changes + Snowflake DDL + Tableau updates in one project
- **Gets smarter over time**: After 3-5 projects, agents know YOUR specific patterns
- **Team learning**: One person's solutions become everyone's knowledge
- **Any tools work**: Pre-built agents for common tools, create custom for yours

**Who this helps:**
- **Analytics engineers**: Coordinate dbt models + warehouse objects + BI changes
- **Data engineers**: Orchestrate pipeline + ingestion + transformation workflows
- **Data architects**: Design cross-system patterns with documented decisions
- **Solo practitioners**: Structure for managing multiple data projects + build expertise
- **Teams**: Shared workflow, consistent patterns, collective learning, easy onboarding

---

## Core Scripts

Essential scripts (automatically called by slash commands):
- `idea.sh` ‚Üí Creates GitHub issues (called by `/idea`)
- `start.sh` ‚Üí Project initialization (called by `/start`)
- `research.sh` ‚Üí Research helper (called by `/research`)
- `switch.sh` ‚Üí Context switching (called by `/switch`)
- `finish.sh` ‚Üí Project completion (called by `/complete`)
- `work-init.sh` ‚Üí Project setup (called by start.sh)
- `pull-all-repos.sh` ‚Üí Sync multiple repos
- `get-repo-owner.sh` ‚Üí Helper utility
- `idea.sh` ‚Üí GitHub issue creation utility

---

## Project Structure

When you `/start` a project, the following structure is created:
```
projects/active/feature-project-name/
‚îú‚îÄ‚îÄ README.md      # Navigation hub
‚îú‚îÄ‚îÄ spec.md        # Requirements from GitHub issue
‚îú‚îÄ‚îÄ context.md     # Dynamic state tracking
‚îî‚îÄ‚îÄ tasks/         # Agent coordination
    ‚îú‚îÄ‚îÄ current-task.md
    ‚îî‚îÄ‚îÄ [tool]-findings.md
```

**Note:** The `projects/` directory is created on-demand. After cleanup, it doesn't exist until your first `/start` command.

When completed with `/complete`, projects are archived to:
```
projects/completed/YYYY-MM/[project-name]/
```

---

## Memory System

The Claude Analytics Framework includes a sophisticated memory system that learns from your work:

### Automatic Pattern Extraction
When you `/complete` a project, the system automatically:
- Extracts reusable patterns from agent findings
- Organizes by role and specialist agent
- Makes patterns available for future projects

### Pattern Markers
Use these markers in `tasks/[tool]-findings.md` for automatic extraction:
```markdown
PATTERN: [Description of reusable pattern]
SOLUTION: [Specific solution that worked]
ERROR-FIX: [Error message] -> [Fix that resolved it]
ARCHITECTURE: [System design pattern]
INTEGRATION: [Cross-system coordination approach]
```

### Knowledge Structure
```
.claude/
‚îú‚îÄ‚îÄ rules/                      # Simple conventions & patterns (auto-loaded)
‚îú‚îÄ‚îÄ skills/
‚îÇ   ‚îú‚îÄ‚îÄ reference-knowledge/   # Deep knowledge for agents (user-invocable: false)
‚îÇ   ‚îî‚îÄ‚îÄ workflows/             # Procedural automation
‚îî‚îÄ‚îÄ agents/                     # Agent definitions
```

**Result:** Every project makes your AI assistant smarter.

---

## Documentation

- `CLAUDE.md` - Project instructions for Claude
- `knowledge/da-agent-hub/` - Platform documentation organized by ADLC phase:
  - `planning/` - Idea management and strategic planning
  - `development/` - Local development and agent coordination
  - `operations/` - Automated operations and troubleshooting
  - `architecture/` - System design and agent capabilities
- `.claude/agents/` - Agent definitions and prompts

**Entry Point:** Start with `knowledge/da-agent-hub/README.md` for comprehensive documentation.

---

## Troubleshooting

### "Permission denied" on scripts
```bash
chmod +x scripts/*.sh
```

### "Command not found: claude"
You need to install Claude Code first. Follow the [official installation guide](https://docs.claude.com/en/docs/claude-code/installation).

See the [Claude Code docs](https://docs.claude.com/en/docs/claude-code) for Windows installation.

### "gh: command not found"
Install GitHub CLI:
```bash
# macOS
brew install gh

# Linux/Windows
# See: https://github.com/cli/cli#installation
```

### "Repository not found" or GitHub authentication issues
```bash
# Authenticate with GitHub CLI
gh auth login

# Verify you have access to your repository
gh repo view dylpickledev/claude-analytics-framework
```

### Setup script fails or MCP not working
```bash
# Verify prerequisites
claude --version    # Should show Claude Code version
gh --version        # Should show GitHub CLI version
node --version      # Should show Node.js version (if using MCP)

# Re-run setup
./setup.sh

# Check MCP configuration (if using MCP)
cat ~/.config/Claude/claude_desktop_config.json
```

---

## Git Workflow

**Protected branches:** Never push directly to main/master/production
- Always create feature branch
- All code changes require PR workflow
- Use commands: `/start` creates branch, `/complete` guides PR creation

**Multi-repo support:**
- Smart context resolution for GitHub operations
- Automatic owner/repo detection
- Cross-repo coordination for complex changes

---

## Customization & Extension

**"Can I customize how this works?"** Yes. Everything is configurable.

### Common Customizations

**Change branch naming:**
```bash
# Edit scripts/start.sh, line ~42
BRANCH="feature/${PROJECT_NAME}"  # Change to your convention
```

**Change project structure:**
```bash
# Edit scripts/work-init.sh
# Modify the template files created in projects/active/
```

**Add your own agent:**
```bash
# Copy a template
cp .claude/agents/specialists/specialist-template.md .claude/agents/specialists/your-tool-expert.md
# Edit with your tool's patterns
```

**Change git workflow:**
```bash
# Edit .claude/rules/git-workflow-patterns.md
# Modify protected branches, naming conventions, etc.
```

**Update tech stack:**
```bash
# Re-run setup or manually edit
nano .claude/config/tech-stack.json
```

### What You Should Customize

**Recommended to customize for your team:**
- `config/repositories.json` ‚Üí Your actual repository URLs
- `.claude/config/tech-stack.json` ‚Üí Your specific tools
- Agent prompts in `.claude/agents/` ‚Üí Your team's patterns and standards
- Branch naming in `scripts/start.sh` ‚Üí Match your conventions

**Safe to leave as-is:**
- Core workflow scripts (unless you know what you're doing)
- Agent architecture (role ‚Üí specialist delegation)
- Memory system patterns

**The framework is designed to be extended, not locked in.**

---

## Creating Custom Agents

The system includes templates for creating new agents:

### Role Agent Template
Copy `.claude/agents/roles/role-template.md` when creating agents that:
- Handle 80% of domain work independently
- Delegate to specialists for edge cases
- Have broad domain expertise

### Specialist Agent Template
Copy `.claude/agents/specialists/specialist-template.md` when creating agents that:
- Provide deep expertise in specific tools/domains
- Are consulted by role agents for complex cases
- May have MCP tool integrations

See `knowledge/da-agent-hub/development/agent-development.md` for detailed guidance.

---

## Advanced Features

### Multi-Repository Synchronization
Keep all your data stack repos in sync:
```bash
./scripts/pull-all-repos.sh
# Updates: dbt_cloud, snowflake utilities, tableau configs, etc.
```

### Conversation Pause/Resume
Save and resume Claude conversations:
```bash
claude /pause "Working on customer analytics dashboard optimization"
# Later...
claude "Resume work on customer analytics"
```

---

## Continuous Improvement

### Learning from Projects
Every completed project contributes to the system:
- Agent capabilities enhanced with new patterns
- Knowledge base expanded with proven solutions
- Specialist expertise refined with production experience

### Improvement PR Workflow
When completing projects, the system recommends:
- Agent updates for high-impact patterns
- Knowledge documentation for recurring topics
- Process improvements for workflow optimization

**Example:**
```bash
claude /complete feature-customer-analytics
# ‚Üí Suggests: "Create improvement PR for analytics-engineer-role
#    with customer metric patterns?"
```

---

## How Teams Iteratively Improve with This

**The self-improving loop:**

```
Project 1: "Build customer dashboard"
  ‚Üì Use basic dbt-expert agent
  ‚Üì Complete project with /complete
  ‚úÖ Extracts: "Incremental model pattern for customer_dim table"

Project 2: "Add customer churn metrics"
  ‚Üì Agent now knows your customer_dim pattern
  ‚Üì Suggests consistent approach
  ‚Üì Complete with /complete
  ‚úÖ Extracts: "Churn calculation logic + dbt test pattern"

Project 3: "Optimize slow customer models"
  ‚Üì Agent knows your patterns + previous optimizations
  ‚Üì Suggests specific improvements for YOUR models
  ‚úÖ Extracts: "Performance patterns for customer analytics"

Future projects: Agent is now an expert in YOUR customer analytics approach
```

**What gets captured and improved:**

1. **Tool-specific patterns** (stored in `.claude/agents/specialists/`)
   - dbt model structures you use
   - SQL optimization patterns that work in your warehouse
   - BI dashboard layouts that match your standards
   - Pipeline orchestration patterns

2. **Cross-system workflows** (stored in `.claude/skills/reference-knowledge/`)
   - How you coordinate dbt + Snowflake + Tableau changes
   - Testing strategies across your stack
   - Deployment procedures
   - Troubleshooting approaches

3. **Team knowledge** (stored in `knowledge/`)
   - Architecture decisions and why
   - Data model documentation
   - Integration patterns
   - Lessons learned from incidents

**How teams benefit:**

- **Onboarding**: New team members get agents that know your patterns
- **Consistency**: Everyone's AI suggests the same proven approaches
- **Documentation**: Knowledge captured automatically, not as separate task
- **Continuous improvement**: Each project makes the next one faster
- **Collective learning**: One person's solution becomes team knowledge

**The improvement workflow:**

```bash
# After completing a project
claude /complete feature-customer-analytics
# Output:
#   ‚úÖ Extracted 3 new patterns to dbt-expert
#   üí° Suggestion: Update analytics-engineer-role with customer modeling patterns?

# Review project documentation in projects/active/customer-analytics/

# Decide: Extract to permanent knowledge OR archive
# Option A: Extract to .claude/skills/reference-knowledge/ for reusable patterns
# Option B: Archive in projects/archive/ for historical reference

# Update agent (if pattern is valuable long-term)
claude "Add the customer_dim incremental pattern to dbt-expert agent"
# Agent now has this pattern permanently

# Commit improvements back to the framework repo
git add .claude/agents/specialists/dbt-expert.md
git commit -m "feat: Add customer dimension modeling patterns to dbt-expert"
# Your team's agents get smarter
```

**Example: Improving dbt knowledge over time**

```markdown
<!-- .claude/agents/specialists/dbt-expert.md - Evolution over time -->

# Initial setup (Day 1):
- Generic dbt best practices
- Common incremental strategies
- Standard testing patterns

# After 5 projects (Month 1):
+ Your specific dbt project structure
+ Custom macro patterns you use
+ Your incremental model conventions
+ Tests that matter for YOUR data quality

# After 20 projects (Month 3):
+ Performance optimization patterns for YOUR warehouse
+ Edge cases and fixes from production incidents
+ Integration patterns with YOUR BI tools
+ Deployment procedures for YOUR CI/CD pipeline

# Result: dbt-expert now = Generic dbt knowledge + YOUR team's expertise
```

**This works for ANY tool:**
- Start with generic agent or create custom one
- Use ADLC workflow (`/start` ‚Üí work ‚Üí `/complete`)
- Framework extracts patterns automatically
- Agents improve with each completed project
- No tool is "unsupported" - you just start with a blank slate and build expertise

---

## How It Works (Behind the Scenes)

**Role-based agents (80/20 pattern):**
- Role agents (analytics-engineer, data-engineer, architect) handle most work
- They delegate to specialists (dbt-expert, snowflake-expert) for complex cases
- Minimizes token usage while maintaining quality

**Dynamic setup:**
- Setup script asks about YOUR tools
- Creates only relevant specialist agents
- Offers to build custom agents for tools not in library
- Saves config to `.claude/config/tech-stack.json`

**MCP integration (optional):**
- Connects Claude to your real systems
- Setup guides credential collection
- Agents query actual data instead of guessing
- You control what systems to connect

**Memory system:**
- Completed projects automatically extracted to pattern library
- Agents learn from your work over time
- Future projects benefit from past learnings
- Pattern markers in task files trigger extraction
- Recent patterns (30 days) vs permanent agent knowledge

---

## Common Questions

**Q: Do I need to know how to code?**
A: Basic comfort with command line and Git helps. The AI agents write code for you, but you should understand what they're doing.

**Q: What if my tool isn't supported?**
A: Setup offers to create a custom specialist agent from a template. You fill in tool-specific knowledge, and the agent works like the built-in ones.

**Q: Is MCP required?**
A: No! MCP is optional. The framework works fine without it - you just get more generic AI advice instead of insights from your live systems.

**Q: Can I use this without GitHub?**
A: Technically yes, but you'll lose issue tracking and some commands. Git is required for branch management.

**Q: Does this work with Claude Desktop or just Claude Code CLI?**
A: Primarily designed for Claude Code CLI. MCP servers can be configured for Claude Desktop, but the slash commands are Claude Code specific.

**Q: How is this different from just using Claude Code normally?**
A: Structure + Learning. This is a meta-repository that orchestrates work across your existing repos (dbt, pipelines, BI, etc.). Instead of ad-hoc conversations scattered across your data stack, you get organized projects that coordinate multi-repo changes, agents that learn YOUR patterns, and automatic knowledge capture. Think: mission control for your data work.

**Q: Do I need to change how I organize my existing repos?**
A: No! This sits alongside your existing repositories. Your dbt project stays in its repo, your Tableau dashboards in theirs, etc. This framework just provides coordination and AI guidance ACROSS those repos.

**Q: Will this work with my team?**
A: Yes - projects create feature branches, use GitHub issues for tracking, and support PR workflow. Each team member runs setup for their own agent configuration.

**Q: What happens to my data?**
A: Nothing leaves your machine except normal Claude API calls. MCP servers run locally and access your systems with credentials YOU provide.

---

## License

MIT License - See LICENSE file

---

## Credits

Built for data/analytics engineers who want AI that understands their modern data stack.

**Inspired by:**
- [dbt Analytics Development Lifecycle](https://www.getdbt.com/analytics-engineering/transformation/)
- [Anthropic's Claude Code](https://docs.claude.com/en/docs/claude-code)
- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)

---

## Get Started

```bash
# 1. Install Claude Code first (if you haven't)
# https://docs.claude.com/en/docs/claude-code/installation

# 2. Clone and setup
git clone https://github.com/dylpickledev/claude-analytics-framework.git
cd claude-analytics-framework
./setup.sh

# Start your first project
claude /start "your first idea"
```

**Questions?** Open an issue or check the [documentation](knowledge/da-agent-hub/README.md)
