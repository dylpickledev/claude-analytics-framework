# dbt Project Learning System

## Overview

The dbt Project Learning System enables dbt-expert to become truly expert in a user's specific dbt project by automatically analyzing project structure, conventions, and patterns during setup. This ensures all dbt recommendations align with the user's existing practices rather than generic best practices.

## Problem Statement

**Current Challenge**: 100% of DA Agent Hub users will have their own dbt project with:
- Unique naming conventions (e.g., `stg_jde_prod__f4111` vs standard patterns)
- Custom macros and packages
- Specific materialization strategies
- Project-specific testing patterns
- Known issues and workarounds
- Particular dbt version (Core vs Cloud)

**Result**: Generic dbt-expert recommendations often don't match user's actual project patterns, requiring corrections and reducing trust.

**Goal**: Make dbt-expert aware of USER'S specific project so recommendations are immediately applicable and follow established conventions.

## Solution Architecture

### Three-Phase Learning System

#### Phase 1: Discovery During Setup (Integrated into `/setup`)
Automatic detection and analysis when user confirms dbt usage.

#### Phase 2: Project Profile Generation
Structured analysis creating `.claude/project-profiles/dbt-project-profile.yaml` with project-specific context.

#### Phase 3: Expert Integration
dbt-expert loads profile before all tasks and provides project-aware recommendations.

## Implementation Details

### 1. Setup Integration

**Location**: `.claude/commands/setup.md`

**New Section After Tool Discovery**:

```markdown
### 5.5. dbt Project Learning

If user confirms dbt usage, trigger project discovery:

**Detection**:
1. Ask user for dbt project location
2. Auto-suggest from common locations:
   - ~/dbt/
   - ~/projects/
   - Path from repositories.json (transformation.dbt_cloud)
   - Current working directory

3. Detect dbt type and version:
   - Check dbt_project.yml
   - Run `dbt --version` if available
   - Check for dbt Cloud API config

**User Experience**:
```
I see you're using dbt! Let me analyze your project to give better recommendations.

Where is your dbt project located?
‚Üí Auto-detected: ~/projects/dbt_cloud

Analyzing project...
‚úÖ Found 2,847 models
‚úÖ Detected custom naming patterns
‚úÖ Identified 12 custom macros
‚úÖ Analyzed testing strategies

Creating dbt project profile... Done!
```

**Execution**:
```bash
# Call new script
./scripts/analyze-dbt-project.sh <project_path>

# Generates: .claude/project-profiles/dbt-project-profile.yaml
```
```

### 2. Project Profile Schema

**File**: `.claude/project-profiles/dbt-project-profile.yaml`

```yaml
# dbt Project Profile - Auto-generated by /setup
# Last updated: 2024-01-20T10:30:00Z

metadata:
  project_name: "graniterock_dbt"
  project_path: "~/workspace/dbt_cloud"
  analyzed_date: "2024-01-20"
  manifest_version: "v11"

dbt_environment:
  type: "cloud"  # or "core"
  version: "1.7.4"
  target: "dbt_dw"  # staging environment
  profile_name: "graniterock"

project_structure:
  layers:
    - staging      # 1,234 models
    - intermediate # 456 models
    - marts        # 1,157 models

  model_counts:
    total: 2847
    by_layer:
      staging: 1234
      intermediate: 456
      marts: 1157
    by_materialization:
      view: 1281
      table: 854
      incremental: 234
      ephemeral: 142

naming_conventions:
  staging: "stg_{source}__{table}"
  facts: "fact_{business_process}"
  dimensions: "dm_{entity}"
  intermediate: "int_{entity}s_{verb}s"

  # Examples detected from project
  staging_examples:
    - stg_jde_prod__f4111
    - stg_shopify__orders
    - stg_greenhouse__applications

  facts_examples:
    - fact_fuel_truck_detail
    - fact_sales_orders
    - fact_safety_inspections

  dimensions_examples:
    - dm_customers
    - dm_products
    - dm_locations

sources:
  primary_sources:
    - name: jde_prod
      tables: 234
    - name: jde_dta
      tables: 89
    - name: shopify
      tables: 45
    - name: greenhouse
      tables: 23

  source_naming: "{source}__{table}"

materializations:
  default: "view"
  by_layer:
    staging: "view"
    intermediate: "ephemeral"
    marts: "table"

  incremental:
    count: 234
    strategy: "merge"
    unique_key_pattern: "{model_name}_key"
    common_configs:
      - on_schema_change: "fail"
      - incremental_predicates: true

macros:
  custom:
    - name: generate_schema_name
      description: "Custom schema naming for environments"
    - name: custom_surrogate_key
      description: "Generates surrogate keys with filename"
    - name: lag_with_default
      description: "LAG function with default handling"

  packages:
    dbt_utils:
      version: "1.1.1"
      commonly_used:
        - surrogate_key
        - get_column_values
        - union_relations
    dbt_expectations:
      version: "0.10.0"
      commonly_used:
        - expect_column_values_to_be_unique
        - expect_table_row_count_to_be_between
    codegen:
      version: "0.12.1"
      commonly_used:
        - generate_model_yaml
        - generate_source

testing:
  coverage: "high"

  patterns:
    primary_key: "unique + not_null on {model_name}_key"
    foreign_key: "relationships test to parent table"
    enum_columns: "accepted_values for status fields"
    business_logic: "custom singular tests in tests/"

  generic_test_usage:
    unique: 2847  # Every model
    not_null: 3456  # Primary keys + critical columns
    relationships: 1234  # Foreign keys
    accepted_values: 567  # Enum columns

  singular_tests:
    - validate_jde_crosswalk
    - check_fuel_calculations
    - validate_safety_inspection_dates

field_conventions:
  primary_keys: "{model_name}_key"
  foreign_keys: "{referenced_table}_key"
  date_columns: "{event}_date"
  timestamp_columns: "{event}_at"
  boolean_columns: "is_{condition}, has_{attribute}"
  amount_columns: "{type}_amount"

known_issues:
  - pattern: "Duplicate keys in staging models"
    cause: "Missing filename in surrogate_key macro"
    fix: "Add source_name to surrogate_key call"
    frequency: "common"

  - pattern: "LAG function errors in incremental models"
    cause: "Window function timing with incremental logic"
    fix: "Use full-refresh when LAG logic changes"
    frequency: "occasional"

  - pattern: "JDE vs Snowflake schema discrepancies"
    cause: "Different column naming in source vs warehouse"
    fix: "Use crosswalk validation test"
    frequency: "common"

semantic_layer:
  enabled: true
  location: "models/semantic_models/"

  metrics:
    count: 47
    categories:
      - revenue_metrics
      - safety_metrics
      - operations_metrics

  dimensions:
    common:
      - date_day
      - business_unit
      - location
      - product_category

performance_notes:
  - "Incremental models avg 10min runtime"
  - "Full project build: 45 minutes"
  - "CI runs use slim-ci pattern (5 minutes)"
  - "Largest models: fact_fuel_truck_detail (2.3M rows)"

learnings:
  # Populated over time as dbt-expert works
  - date: "2024-01-20"
    category: "setup"
    finding: "Initial project profile created"
    context: "Setup phase analysis"

# Future additions from dbt-expert interactions
# - date: "YYYY-MM-DD"
#   category: "performance|testing|patterns|errors"
#   finding: "What was learned"
#   pattern: "Reusable solution"
#   confidence: "high|medium|low"
```

### 3. Analysis Script

**File**: `scripts/analyze-dbt-project.sh`

```bash
#!/bin/bash
# Analyzes dbt project structure and creates project profile

PROJECT_PATH="$1"
PROFILE_PATH=".claude/project-profiles/dbt-project-profile.yaml"

# Validation
if [ ! -f "$PROJECT_PATH/dbt_project.yml" ]; then
  echo "‚ùå Not a valid dbt project (missing dbt_project.yml)"
  exit 1
fi

echo "üîç Analyzing dbt project at: $PROJECT_PATH"

# Use Python script for detailed analysis
python3 scripts/analyze_dbt_project.py "$PROJECT_PATH" "$PROFILE_PATH"

if [ $? -eq 0 ]; then
  echo "‚úÖ Project profile created: $PROFILE_PATH"
  echo ""
  echo "Summary:"
  grep "total:" "$PROFILE_PATH" | head -5
else
  echo "‚ùå Analysis failed"
  exit 1
fi
```

**File**: `scripts/analyze_dbt_project.py`

```python
#!/usr/bin/env python3
"""
Analyzes dbt project structure and generates project profile.
"""

import os
import sys
import yaml
import json
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime

def analyze_dbt_project(project_path: str) -> dict:
    """Main analysis function."""

    project_path = Path(project_path)

    # Read dbt_project.yml
    with open(project_path / "dbt_project.yml") as f:
        dbt_project = yaml.safe_load(f)

    # Try to read manifest.json (if exists)
    manifest_path = project_path / "target" / "manifest.json"
    manifest = None
    if manifest_path.exists():
        with open(manifest_path) as f:
            manifest = json.load(f)

    profile = {
        "metadata": analyze_metadata(project_path, dbt_project),
        "dbt_environment": analyze_environment(dbt_project),
        "project_structure": analyze_structure(project_path, manifest),
        "naming_conventions": detect_naming_conventions(project_path, manifest),
        "sources": analyze_sources(project_path, manifest),
        "materializations": analyze_materializations(manifest),
        "macros": analyze_macros(project_path, dbt_project),
        "testing": analyze_testing(project_path, manifest),
        "field_conventions": detect_field_conventions(manifest),
        "known_issues": [],  # Populated over time
        "semantic_layer": analyze_semantic_layer(project_path),
        "performance_notes": [],  # Populated over time
        "learnings": [{
            "date": datetime.now().strftime("%Y-%m-%d"),
            "category": "setup",
            "finding": "Initial project profile created",
            "context": "Setup phase analysis"
        }]
    }

    return profile

def analyze_metadata(project_path: Path, dbt_project: dict) -> dict:
    """Extract project metadata."""
    return {
        "project_name": dbt_project.get("name", "unknown"),
        "project_path": str(project_path),
        "analyzed_date": datetime.now().strftime("%Y-%m-%d"),
        "manifest_version": "detected from manifest"
    }

def analyze_environment(dbt_project: dict) -> dict:
    """Detect dbt environment details."""
    # Try to detect dbt version
    # Check for dbt Cloud indicators
    # This is simplified - full implementation would check profiles.yml, etc.
    return {
        "type": "cloud",  # or "core"
        "version": "detected",
        "target": "production",
        "profile_name": dbt_project.get("profile", "default")
    }

def analyze_structure(project_path: Path, manifest: dict) -> dict:
    """Analyze project layer structure."""
    if not manifest:
        # Fallback: scan filesystem
        return detect_structure_from_filesystem(project_path)

    # Use manifest for accurate counts
    models = manifest.get("nodes", {})
    model_nodes = {k: v for k, v in models.items() if k.startswith("model.")}

    # Detect layers from paths
    layers = defaultdict(int)
    materializations = defaultdict(int)

    for node_id, node in model_nodes.items():
        # Extract layer from path
        path = node.get("path", "")
        if "staging" in path:
            layers["staging"] += 1
        elif "intermediate" in path or "int_" in path:
            layers["intermediate"] += 1
        elif "marts" in path or "mart" in path:
            layers["marts"] += 1

        # Count materializations
        config = node.get("config", {})
        mat = config.get("materialized", "view")
        materializations[mat] += 1

    total = len(model_nodes)

    return {
        "layers": list(layers.keys()),
        "model_counts": {
            "total": total,
            "by_layer": dict(layers),
            "by_materialization": dict(materializations)
        }
    }

def detect_naming_conventions(project_path: Path, manifest: dict) -> dict:
    """Detect naming patterns from model names."""
    if not manifest:
        return {}

    models = manifest.get("nodes", {})
    model_nodes = {k: v for k, v in models.items() if k.startswith("model.")}

    staging_examples = []
    fact_examples = []
    dim_examples = []

    for node_id, node in model_nodes.items():
        name = node.get("name", "")
        path = node.get("path", "")

        if "staging" in path and name.startswith("stg_"):
            staging_examples.append(name)
        elif name.startswith("fact_") or name.startswith("fct_"):
            fact_examples.append(name)
        elif name.startswith("dim_") or name.startswith("dm_"):
            dim_examples.append(name)

    # Detect patterns
    staging_pattern = detect_pattern(staging_examples[:10])
    fact_pattern = detect_pattern(fact_examples[:10])
    dim_pattern = detect_pattern(dim_examples[:10])

    return {
        "staging": staging_pattern or "stg_{source}__{table}",
        "facts": fact_pattern or "fact_{business_process}",
        "dimensions": dim_pattern or "dim_{entity}",
        "staging_examples": staging_examples[:5],
        "facts_examples": fact_examples[:5],
        "dimensions_examples": dim_examples[:5]
    }

def detect_pattern(examples: list) -> str:
    """Detect naming pattern from examples."""
    if not examples:
        return None

    # Simple pattern detection
    # This is simplified - full implementation would be more sophisticated
    first = examples[0]

    if "__" in first:
        return "{source}__{table}"
    elif first.count("_") >= 2:
        return "{prefix}_{entity}"
    else:
        return None

def analyze_sources(project_path: Path, manifest: dict) -> dict:
    """Analyze source definitions."""
    if not manifest:
        return {}

    sources = manifest.get("sources", {})

    source_summary = defaultdict(int)
    for source_id, source in sources.items():
        source_name = source.get("source_name", "unknown")
        source_summary[source_name] += 1

    primary_sources = [
        {"name": name, "tables": count}
        for name, count in sorted(source_summary.items(), key=lambda x: x[1], reverse=True)
    ]

    return {
        "primary_sources": primary_sources[:10],
        "source_naming": "{source}__{table}"  # Detect from examples
    }

def analyze_materializations(manifest: dict) -> dict:
    """Analyze materialization strategies."""
    if not manifest:
        return {}

    models = manifest.get("nodes", {})
    model_nodes = {k: v for k, v in models.items() if k.startswith("model.")}

    incremental_count = sum(
        1 for node in model_nodes.values()
        if node.get("config", {}).get("materialized") == "incremental"
    )

    return {
        "default": "view",
        "incremental": {
            "count": incremental_count,
            "strategy": "merge",  # Could detect from config
            "unique_key_pattern": "{model_name}_key"
        }
    }

def analyze_macros(project_path: Path, dbt_project: dict) -> dict:
    """Analyze custom macros and packages."""

    # Scan macros directory
    macros_path = project_path / "macros"
    custom_macros = []

    if macros_path.exists():
        for macro_file in macros_path.glob("*.sql"):
            custom_macros.append({
                "name": macro_file.stem,
                "description": f"Custom macro from {macro_file.name}"
            })

    # Check packages
    packages = {}
    packages_file = project_path / "packages.yml"
    if packages_file.exists():
        with open(packages_file) as f:
            packages_config = yaml.safe_load(f)
            for package in packages_config.get("packages", []):
                if "package" in package:
                    pkg_name = package["package"].split("/")[-1]
                    packages[pkg_name] = {
                        "version": package.get("version", "latest")
                    }

    return {
        "custom": custom_macros[:10],
        "packages": packages
    }

def analyze_testing(project_path: Path, manifest: dict) -> dict:
    """Analyze testing patterns."""
    if not manifest:
        return {}

    tests = manifest.get("nodes", {})
    test_nodes = {k: v for k, v in tests.items() if k.startswith("test.")}

    generic_tests = defaultdict(int)
    singular_tests = []

    for test_id, test in test_nodes.items():
        test_type = test.get("test_metadata", {}).get("name")
        if test_type:
            generic_tests[test_type] += 1
        else:
            # Singular test
            singular_tests.append(test.get("name", "unknown"))

    return {
        "coverage": "high" if len(test_nodes) > 100 else "medium",
        "generic_test_usage": dict(generic_tests),
        "singular_tests": singular_tests[:10]
    }

def detect_field_conventions(manifest: dict) -> dict:
    """Detect field naming conventions."""
    # Simplified - would analyze column names from manifest
    return {
        "primary_keys": "{model_name}_key",
        "foreign_keys": "{referenced_table}_key",
        "date_columns": "{event}_date",
        "timestamp_columns": "{event}_at",
        "boolean_columns": "is_{condition}, has_{attribute}"
    }

def analyze_semantic_layer(project_path: Path) -> dict:
    """Check for semantic layer usage."""
    semantic_path = project_path / "models" / "semantic_models"

    if semantic_path.exists():
        return {
            "enabled": True,
            "location": "models/semantic_models/",
            "metrics": {"count": len(list(semantic_path.glob("*.yml")))}
        }

    return {"enabled": False}

def detect_structure_from_filesystem(project_path: Path) -> dict:
    """Fallback: detect structure from filesystem."""
    models_path = project_path / "models"

    if not models_path.exists():
        return {}

    layers = []
    if (models_path / "staging").exists():
        layers.append("staging")
    if (models_path / "intermediate").exists():
        layers.append("intermediate")
    if (models_path / "marts").exists():
        layers.append("marts")

    return {
        "layers": layers,
        "model_counts": {
            "total": "unknown (run dbt compile first)",
            "by_layer": {}
        }
    }

def main():
    if len(sys.argv) != 3:
        print("Usage: analyze_dbt_project.py <project_path> <output_path>")
        sys.exit(1)

    project_path = sys.argv[1]
    output_path = sys.argv[2]

    try:
        profile = analyze_dbt_project(project_path)

        # Write YAML
        with open(output_path, 'w') as f:
            yaml.dump(profile, f, default_flow_style=False, sort_keys=False)

        print(f"‚úÖ Project profile created: {output_path}")

    except Exception as e:
        print(f"‚ùå Error analyzing project: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

### 4. dbt-expert Integration

**Update**: `.claude/agents/specialists/dbt-expert.md`

Add to Memory Check Protocol section:

```markdown
## Memory Check Protocol (ENHANCED)

Before beginning ANY dbt analysis, follow this sequence:

### 0. Load Project Profile (NEW - ALWAYS FIRST)
```bash
# Check for project profile
if [ -f .claude/project-profiles/dbt-project-profile.yaml ]; then
  # Load project-specific context
  - Project naming conventions
  - Known materialization strategies
  - Custom macros and packages
  - Common issues and fixes
  - dbt version and capabilities
fi
```

**Project Profile Awareness**:
- ‚úÖ Use user's naming conventions in all recommendations
- ‚úÖ Reference user's custom macros when applicable
- ‚úÖ Check dbt version before suggesting features
- ‚úÖ Consider known issues before generic troubleshooting
- ‚úÖ Align test recommendations with existing patterns

### Project-Aware Recommendation Pattern

**Before (Generic)**:
```
Recommendation: Create staging model using standard pattern:
stg_{source}_{table}
```

**After (Project-Aware)**:
```
Recommendation: Create staging model following YOUR project convention:
stg_jde_prod__f4111 (detected pattern: stg_{source}__{table})

This matches your existing staging models like:
- stg_shopify__orders
- stg_greenhouse__applications
```

### Version-Aware Feature Suggestions

**Check version compatibility**:
```markdown
# Before suggesting dbt features
Project dbt version: 1.7.4

‚úÖ SAFE to recommend:
- data_tests (available in 1.7+)
- incremental_predicates (available in 1.6+)
- semantic layer (if dbt Cloud)

‚ùå AVOID recommending:
- New 1.8 features (user on 1.7)
- dbt Cloud features (user on Core)
```

### 1. Recent Patterns
[existing content...]

### 2. Domain Patterns
[existing content...]
```

## User Experience Flow

### Setup Phase

```
User: /setup

Claude: What tools are you using?
User: dbt Core, Snowflake, Tableau

Claude: Great! Where is your dbt project?
User: ~/workspace/dbt_cloud

Claude: üîç Analyzing your dbt project...

Found:
‚úÖ 2,847 models (staging, intermediate, marts)
‚úÖ Custom naming: stg_jde_prod__f4111 pattern
‚úÖ 12 custom macros (custom_surrogate_key, lag_with_default, etc.)
‚úÖ 234 incremental models using merge strategy
‚úÖ dbt version 1.7.4 (Cloud)

Creating dbt project profile... ‚úÖ Done!

dbt-expert now understands YOUR project conventions!

Try it: "Why is dim_customers slow?"
(I'll analyze using your project context)
```

### Working with dbt-expert

```
User: "Create a new staging model for the new Salesforce source"

Claude (dbt-expert):
I'll create a staging model following YOUR project's conventions:

File: models/staging/salesforce/stg_salesforce__accounts.sql

Following your patterns:
‚úÖ Naming: stg_{source}__{table} (matches your existing stg_jde_prod__f4111)
‚úÖ Primary key: stg_salesforce__accounts_key (matches your {model}_key pattern)
‚úÖ Source reference: {{ source('salesforce', 'accounts') }}
‚úÖ Tests: unique + not_null on primary key (your standard pattern)

This aligns with your 1,234 existing staging models.
```

## Benefits

### For Users
1. **Immediate Accuracy**: Recommendations match existing conventions from day 1
2. **Zero Configuration**: Automatic analysis during setup
3. **Version Safety**: No suggestions for unavailable features
4. **Pattern Consistency**: Enforces established team patterns
5. **Faster Onboarding**: New team members learn project conventions

### For dbt-expert
1. **Context Awareness**: Knows project-specific patterns
2. **Confidence**: Can reference actual project structure
3. **Learning**: Profile grows with each interaction
4. **Efficiency**: Fewer correction cycles

### For Teams
1. **Convention Enforcement**: AI maintains established patterns
2. **Documentation**: Profile serves as project documentation
3. **Knowledge Transfer**: New team members get instant context
4. **Consistency**: All recommendations align with team standards

## Future Enhancements

### v2 Features
- **Package Usage Analysis**: Detect most-used dbt_utils macros
- **Performance Baselines**: Track typical model runtimes
- **Test Coverage Gaps**: Identify untested models
- **Lineage Awareness**: Build dependency graphs

### v3 Features
- **Multi-Project Support**: Handle multiple dbt projects
- **Team Profiles**: Share learnings across team members
- **Pattern Evolution**: Track convention changes over time
- **Automated Updates**: Re-analyze on major changes

### v4 Features
- **Cross-Project Patterns**: Learn from multiple projects
- **Best Practice Scoring**: Rate project against best practices
- **Migration Assistant**: Help upgrade dbt versions
- **Convention Validator**: Check new models against patterns

## Success Metrics

- [ ] 90% of dbt-expert recommendations match user conventions (measured by user corrections)
- [ ] Zero "that's not how we do it" feedback from users
- [ ] Faster issue resolution (baseline: time to first accurate recommendation)
- [ ] New team member productivity (time to first successful contribution)
- [ ] Profile accuracy (percentage of detected patterns that are correct)

## Implementation Checklist

### Phase 1: Core Learning (Week 1)
- [ ] Create `scripts/analyze-dbt-project.sh`
- [ ] Create `scripts/analyze_dbt_project.py`
- [ ] Define `dbt-project-profile.yaml` schema
- [ ] Integrate into `/setup` command
- [ ] Update dbt-expert memory protocol
- [ ] Test on 3 real dbt projects

### Phase 2: Enhanced Analysis (Week 2)
- [ ] Macro inventory and documentation
- [ ] Testing pattern detection
- [ ] Custom test identification
- [ ] Semantic layer discovery
- [ ] Performance baseline capture

### Phase 3: Continuous Learning (Week 3)
- [ ] Profile update mechanism
- [ ] Learning capture from interactions
- [ ] Version upgrade detection
- [ ] Pattern evolution tracking

### Phase 4: Documentation & Launch (Week 4)
- [ ] User guide for project learning
- [ ] Troubleshooting documentation
- [ ] Example profiles from test projects
- [ ] Release notes and announcement

## Maintenance

### Profile Updates
- **Automatic**: When dbt version changes, project structure changes
- **Manual**: User can run `/setup` again to re-analyze
- **Incremental**: dbt-expert appends learnings during work

### Validation
- Periodically check profile accuracy
- Compare recommendations against actual project
- Collect user feedback on accuracy

### Versioning
- Track profile schema version
- Migrate old profiles to new schema
- Maintain backward compatibility

---

*This system transforms dbt-expert from generic consultant to project-specific expert, ensuring every recommendation aligns with the user's actual dbt project.*
